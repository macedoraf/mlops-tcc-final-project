from datetime import timedelta
from feast import Entity, FeatureView, Field, FileSource, ValueType
from feast.types import Float32, Int64, String

# Define an entity for the review
review = Entity(name="review_id", value_type=ValueType.STRING, description="The ID of the review")

# Define the source of the features (Parquet files generated by Airflow)
# Note: The path should be relative to where the command is run or absolute.
# Since we are running from the root or mapped volumes, we'll use a path that works in the container.
# In the container, data is at /app/data or /opt/airflow/data.
# Let's assume the offline store path is consistent across services via the volume mount.
# We will use a relative path from the feature_repo root if possible, or an absolute path that matches the container layout.
# Given the docker-compose volume mappings:
# API: ./data -> /app/data
# Airflow: ./data -> /opt/airflow/data
# We should try to use a path that is accessible.
# However, Feast 'local' provider usually expects paths relative to the feature store yaml or absolute local paths.
# Since we are running in Docker, we should use the container path.
# Let's assume we run feast commands from /app or /opt/airflow.
# A safe bet is to use the path where we expect the parquet files to be.
batch_source = FileSource(
    path="../data/processed/sentiment_features.parquet",
    timestamp_field="event_timestamp",
    created_timestamp_column="created_timestamp",
)

sentiment_features_view = FeatureView(
    name="sentiment_features_view",
    entities=[review],
    ttl=timedelta(days=36500), # Very long TTL as reviews don't expire quickly
    schema=[
        Field(name="polarity", dtype=Int64),
        Field(name="text", dtype=String),
    ],
    online=True,
    source=batch_source,
    tags={"team": "mlops"},
)
